"""
–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–¥–∞–Ω–∏—è 3.
–° —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –∑–∞—Ä–ø–ª–∞—Ç–µ.
"""
import asyncio
import pandas as pd
from pathlib import Path
import logging
import json
import time
from datetime import datetime
from typing import Dict, Optional, Any, List


class DataPipeline:
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–æ–≤ –¥–∞–Ω–Ω—ã—Ö.
    –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∑–∞—Ä–ø–ª–∞—Ç–µ: –¥–∞–Ω–Ω—ã–µ –ø–æ–ø–∞–¥–∞—é—Ç –≤ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–∞–π–ª,
    –µ—Å–ª–∏ –∑–∞—Ä–ø–ª–∞—Ç–∞ –±–æ–ª—å—à–µ –∏–ª–∏ —Ä–∞–≤–Ω–∞ –∑–∞–¥–∞–Ω–Ω–æ–º—É —É—Å–ª–æ–≤–∏—é.
    """

    def __init__(self, s3_client, config: Dict[str, Any]):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞.

        Args:
            s3_client: –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π S3 –∫–ª–∏–µ–Ω—Ç
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞
        """
        self.s3_client = s3_client
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)

        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫–∏ –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç
        self.watch_folder = Path(config['watch_folder'])
        self.temp_folder = Path(config['temp_folder'])
        self.processed_folder = Path(config['processed_folder'])
        self.log_folder = Path(config['log_folder'])
        self.filter = int(config['filter_threshold'])
        self.max_threshold = int(config['max_threshold'])

        for folder in [self.watch_folder, self.temp_folder,
                       self.processed_folder, self.log_folder]:
            folder.mkdir(parents=True, exist_ok=True)

        self.logger.info(f"–ü–∞–π–ø–ª–∞–π–Ω –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        self.logger.info(f"–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è: –∑–∞—Ä–ø–ª–∞—Ç–∞ > {self.filter}")
        self.logger.info(f"–ü–∞–ø–∫–∞ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è: {self.watch_folder}")
        self.logger.info(f"–ü–∞–ø–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {self.processed_folder}")

    async def process_file(self, file_path: Path) -> Dict[str, Any]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ —á–µ—Ä–µ–∑ –ø–∞–π–ø–ª–∞–π–Ω.

        Returns:
            Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        result = {
            'file_path': str(file_path),
            'file_name': file_path.name,
            'start_time': datetime.now().isoformat(),
            'success': False,
            'error': None,
            'records_processed': 0,
            'records_filtered': 0,
            'filtered_by_salary': 0,  # –ù–æ–≤–æ–µ –ø–æ–ª–µ: –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ –ø–æ –∑–∞—Ä–ø–ª–∞—Ç–µ
            's3_path': None,
            'version_id': None
        }

        try:
            # –®–∞–≥ 1: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–∞
            self.logger.info(f"üìÅ –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞: {file_path.name}")

            if not file_path.exists():
                result['error'] = f"–§–∞–π–ª –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {file_path}"
                self.logger.error(result['error'])
                return result

            file_size = file_path.stat().st_size
            self.logger.info(f"   –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {file_size} –±–∞–π—Ç")

            # –®–∞–≥ 2: –ß—Ç–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ñ–æ—Ä–º–∞—Ç–∞
            df = await self._read_data_file(file_path)
            if df is None:
                result['error'] = f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª: {file_path}"
                self.logger.error(result['error'])
                return result

            result['records_processed'] = len(df)
            self.logger.info(f"   –ü—Ä–æ—á–∏—Ç–∞–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(df)}")

            # –®–∞–≥ 3: –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –∑–∞—Ä–ø–ª–∞—Ç–µ
            processed_df, salary_stats = await self._process_data_with_salary_filter(df)
            result['records_filtered'] = len(processed_df)
            result['filtered_by_salary'] = salary_stats.get('filtered_count', 0)
            result['salary_stats'] = salary_stats

            if len(processed_df) == 0:
                self.logger.warning(f"   –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å")
            else:
                self.logger.info(f"   –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –æ—Å—Ç–∞–ª–æ—Å—å: {len(processed_df)} –∑–∞–ø–∏—Å–µ–π")
                self.logger.info(f"   –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ –ø–æ –∑–∞—Ä–ø–ª–∞—Ç–µ: {salary_stats.get('filtered_count', 0)} –∑–∞–ø–∏—Å–µ–π")

            # –®–∞–≥ 4: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            temp_file = await self._save_temp_file(processed_df, file_path, result)
            if temp_file is None:
                result['error'] = "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª"
                self.logger.error(result['error'])
                return result

            # –®–∞–≥ 5: –ó–∞–≥—Ä—É–∑–∫–∞ –≤ S3
            s3_object_name = (f"processed/"
                              f"{datetime.now().strftime('%Y-%m-%d')}/"
                              f"salary_filtered_{file_path.stem}_{int(time.time())}.csv")

            self.logger.info(f"   üì§ –ó–∞–≥—Ä—É–∑–∫–∞ –≤ S3: {s3_object_name}")
            success = await self.s3_client.upload(str(temp_file), s3_object_name)

            if success:
                # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä—Å–∏—é —Ñ–∞–π–ª–∞
                try:
                    version_id = await self.s3_client.upload_with_versioning(str(temp_file), s3_object_name)
                    result['version_id'] = version_id
                except:
                    result['version_id'] = 'unknown'

                result['s3_path'] = s3_object_name
                result['success'] = True
                self.logger.info(f"   ‚úÖ –§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω –≤ S3: {s3_object_name}")

                # –®–∞–≥ 6: –ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
                await self._move_original_file(file_path)

            else:
                result['error'] = "–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª –≤ S3"
                self.logger.error(result['error'])

            # –®–∞–≥ 7: –£–¥–∞–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            if temp_file.exists():
                temp_file.unlink()
                self.logger.info(f"   üóëÔ∏è  –í—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —É–¥–∞–ª–µ–Ω: {temp_file.name}")

        except Exception as e:
            result['error'] = str(e)
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {file_path.name}: {e}")

        result['end_time'] = datetime.now().isoformat()
        return result

    async def _read_data_file(self, file_path: Path) -> Optional[pd.DataFrame]:
        """
        –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ –¥–∞–Ω–Ω—ã—Ö –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ñ–æ—Ä–º–∞—Ç–∞.
        """
        try:
            ext = file_path.suffix.lower()

            if ext == '.csv':
                # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏
                try:
                    df = pd.read_csv(file_path, encoding='utf-8')
                except:
                    try:
                        df = pd.read_csv(file_path, encoding='cp1251')
                    except:
                        df = pd.read_csv(file_path, encoding='utf-8', errors='replace')
            elif ext == '.json':
                df = pd.read_json(file_path)
            elif ext in ['.xlsx', '.xls']:
                df = pd.read_excel(file_path)
            elif ext == '.parquet':
                df = pd.read_parquet(file_path)
            else:
                # –ü—Ä–æ–±—É–µ–º –∫–∞–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª
                try:
                    df = pd.read_csv(file_path, sep=None, engine='python', encoding='utf-8')
                except:
                    self.logger.error(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: {ext}")
                    return None

            self.logger.info(f"   –§–æ—Ä–º–∞—Ç: {ext}, –∫–æ–ª–æ–Ω–∫–∏: {list(df.columns)}")
            self.logger.info(f"   –†–∞–∑–º–µ—Ä: {df.shape[0]} —Å—Ç—Ä–æ–∫, {df.shape[1]} —Å—Ç–æ–ª–±—Ü–æ–≤")

            return df

        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {file_path}: {e}")
            return None

    async def _process_data_with_salary_filter(self, df: pd.DataFrame) -> tuple[pd.DataFrame, Dict]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ –∑–∞—Ä–ø–ª–∞—Ç–µ.

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            - –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π DataFrame
            - –°—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        """
        if df.empty:
            return df, {'filtered_count': 0, 'salary_columns': []}

        processed_df = df.copy()
        salary_stats = {
            'filtered_count': 0,
            'salary_columns': [],
            'original_count': len(df)
        }

        try:
            # –®–∞–≥ 1: –ü–æ–∏—Å–∫ –∫–æ–ª–æ–Ω–æ–∫ —Å –∑–∞—Ä–ø–ª–∞—Ç–æ–π
            salary_columns = self._find_salary_columns(processed_df)
            salary_stats['salary_columns'] = salary_columns

            if not salary_columns:
                self.logger.warning("   ‚ö†Ô∏è –ö–æ–ª–æ–Ω–∫–∏ —Å –∑–∞—Ä–ø–ª–∞—Ç–æ–π –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                self.logger.info("   –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏:")
                for col in processed_df.columns:
                    col_type = processed_df[col].dtype
                    self.logger.info(f"     - {col} ({col_type})")
                return processed_df, salary_stats

            self.logger.info(f"   –ù–∞–π–¥–µ–Ω—ã –∫–æ–ª–æ–Ω–∫–∏ —Å –∑–∞—Ä–ø–ª–∞—Ç–æ–π: {salary_columns}")

            # –®–∞–≥ 2: –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            initial_count = len(processed_df)

            # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            processed_df = processed_df.drop_duplicates()
            dup_removed = initial_count - len(processed_df)
            if dup_removed > 0:
                self.logger.info(f"   –£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {dup_removed}")

            # –®–∞–≥ 3: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∑–∞—Ä–ø–ª–∞—Ç–µ
            for salary_col in salary_columns:
                if salary_col in processed_df.columns:
                    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —á–∏—Å–ª–æ–≤–æ–π —Ñ–æ—Ä–º–∞—Ç
                    processed_df[salary_col] = pd.to_numeric(processed_df[salary_col], errors='coerce')

                    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–æ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
                    salary_before = processed_df[salary_col].describe()
                    self.logger.info(f"   –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ {salary_col} –¥–æ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏:")
                    self.logger.info(f"     –ú–∏–Ω: {salary_before.get('min', 'N/A'):.2f}")
                    self.logger.info(f"     –ú–∞–∫—Å: {salary_before.get('max', 'N/A'):.2f}")
                    self.logger.info(f"     –°—Ä–µ–¥–Ω–µ–µ: {salary_before.get('mean', 'N/A'):.2f}")

                    # –§–ò–õ–¨–¢–†–ê–¶–ò–Ø: –∑–∞—Ä–ø–ª–∞—Ç–∞ > 100
                    mask = processed_df[salary_col] > self.filter
                    filtered_count = (~mask).sum()

                    if filtered_count > 0:
                        self.logger.info(
                            f"   –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ –∑–∞–ø–∏—Å–µ–π –ø–æ {salary_col} "
                            f"(–∑–∞—Ä–ø–ª–∞—Ç–∞ <= {self.filter}): {filtered_count}")
                        salary_stats['filtered_count'] += int(filtered_count)
                        processed_df = processed_df[mask]
                    else:
                        self.logger.info(f"   –í—Å–µ –∑–∞–ø–∏—Å–∏ –ø–æ {salary_col} –∏–º–µ—é—Ç –∑–∞—Ä–ø–ª–∞—Ç—É > "
                                         f"{self.filter}")

                    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
                    if len(processed_df) > 0:
                        salary_after = processed_df[salary_col].describe()
                        self.logger.info(f"   –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ {salary_col} –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏:")
                        self.logger.info(f"     –ú–∏–Ω: {salary_after.get('min', 'N/A'):.2f}")
                        self.logger.info(f"     –ú–∞–∫—Å: {salary_after.get('max', 'N/A'):.2f}")
                        self.logger.info(f"     –°—Ä–µ–¥–Ω–µ–µ: {salary_after.get('mean', 'N/A'):.2f}")

            # –®–∞–≥ 4: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
            # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ —Å –ø—É—Å—Ç—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –≤ –≤–∞–∂–Ω—ã—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö
            if salary_columns:
                processed_df = processed_df.dropna(subset=salary_columns)
                self.logger.info(f"   –£–¥–∞–ª–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π —Å –∑–∞—Ä–ø–ª–∞—Ç–æ–π <="
                                 f" {self.filter}: {initial_count - len(processed_df)}")

            # –®–∞–≥ 5: –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            self.logger.info(f"   –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
            self.logger.info(f"     –ë—ã–ª–æ –∑–∞–ø–∏—Å–µ–π: {initial_count}")
            self.logger.info(f"     –°—Ç–∞–ª–æ –∑–∞–ø–∏—Å–µ–π: {len(processed_df)}")
            self.logger.info(f"     –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ –ø–æ –∑–∞—Ä–ø–ª–∞—Ç–µ: {salary_stats['filtered_count']}")

            return processed_df, salary_stats

        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
            return df, salary_stats

    def _find_salary_columns(self, df: pd.DataFrame) -> List[str]:
        """
        –ü–æ–∏—Å–∫ –∫–æ–ª–æ–Ω–æ–∫ —Å –∑–∞—Ä–ø–ª–∞—Ç–æ–π –≤ DataFrame.
        """
        salary_keywords = ['salary', '–∑–∞—Ä–ø–ª–∞—Ç–∞', '–æ–∫–ª–∞–¥', 'income', '–¥–æ—Ö–æ–¥', 'pay', 'wage', 'compensation']
        salary_columns = []

        for col in df.columns:
            col_lower = str(col).lower()

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            for keyword in salary_keywords:
                if keyword in col_lower:
                    salary_columns.append(col)
                    break

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –µ—Å–ª–∏ –∫–æ–ª–æ–Ω–∫–∞ —á–∏—Å–ª–æ–≤–∞—è –∏ –∏–º—è –ø–æ—Ö–æ–∂–µ –Ω–∞ –∑–∞—Ä–ø–ª–∞—Ç—É
            if col not in salary_columns and pd.api.types.is_numeric_dtype(df[col]):
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω –∑–Ω–∞—á–µ–Ω–∏–π (–∑–∞—Ä–ø–ª–∞—Ç–∞ –æ–±—ã—á–Ω–æ –≤ —Ä–∞–∑—É–º–Ω—ã—Ö –ø—Ä–µ–¥–µ–ª–∞—Ö)
                try:
                    col_min = df[col].min()
                    col_max = df[col].max()

                    # –ó–∞—Ä–ø–ª–∞—Ç–∞ –æ–±—ã—á–Ω–æ –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –æ—Ç 0 –¥–æ max_threshold
                    if (0 <= col_min <= self.max_threshold
                            and 0 <= col_max <= self.max_threshold):
                        # –ò –∏–º—è –∫–æ–ª–æ–Ω–∫–∏ –Ω–µ –ø–æ—Ö–æ–∂–µ –Ω–∞ ID –∏–ª–∏ –≤–æ–∑—Ä–∞—Å—Ç
                        if not any(x in col_lower for x in ['id', 'age', '–≤–æ–∑—Ä–∞—Å—Ç', '–∫–æ–¥', '–Ω–æ–º–µ—Ä']):
                            salary_columns.append(col)
                except:
                    pass

        return list(set(salary_columns))  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã

    async def _save_temp_file(self, df: pd.DataFrame, original_file: Path, result: Dict) -> Optional[Path]:
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª.
        """
        try:
            # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
            timestamp = int(time.time())
            original_name = original_file.stem

            # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤ –∏–º—è —Ñ–∞–π–ª–∞
            filtered_count = result.get('filtered_by_salary', 0)
            total_count = result.get('records_processed', 0)

            temp_filename = f"salary_filtered_{original_name}_total{total_count}_filtered{filtered_count}_{timestamp}.csv"
            temp_file = self.temp_folder / temp_filename

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
            with open(temp_file, 'w', encoding='utf-8') as f:
                # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
                f.write(f"# –§–∞–π–ª –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω –ø–æ –∑–∞—Ä–ø–ª–∞—Ç–µ (> {self.filter})\n")
                f.write(f"# –ò—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª: {original_file.name}\n")
                f.write(f"# –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"# –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {total_count}\n")
                f.write(f"# –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ –ø–æ –∑–∞—Ä–ø–ª–∞—Ç–µ: {filtered_count}\n")
                f.write(f"# –û—Å—Ç–∞–ª–æ—Å—å –∑–∞–ø–∏—Å–µ–π: {len(df)}\n")
                f.write(f"# –ü–æ—Ä–æ–≥ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: > {self.filter}\n")
                f.write(f"#\n")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
            df.to_csv(temp_file, mode='a', index=False, encoding='utf-8')

            self.logger.info(f"   üìù –í—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {temp_file.name}")
            self.logger.info(f"   üìä –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {temp_file.stat().st_size} –±–∞–π—Ç")

            return temp_file

        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞: {e}")
            return None

    async def _move_original_file(self, file_path: Path) -> None:  # ‚Üê –í–°–¢–ê–í–¨–¢–ï –ó–î–ï–°–¨
        """
        –ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –∏–ª–∏ –∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞.
        """
        try:
            # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –∞—Ä—Ö–∏–≤–∞ –ø–æ –¥–∞—Ç–µ
            archive_date = datetime.now().strftime('%Y-%m-%d')
            archive_folder = self.processed_folder / "archive" / archive_date
            archive_folder.mkdir(parents=True, exist_ok=True)

            # –ö–æ–ø–∏—Ä—É–µ–º —Ñ–∞–π–ª –≤ –∞—Ä—Ö–∏–≤ (–≤–º–µ—Å—Ç–æ –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏—è)
            archive_file = archive_folder / file_path.name

            # –ï—Å–ª–∏ —Ñ–∞–π–ª —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –¥–æ–±–∞–≤–ª—è–µ–º timestamp
            if archive_file.exists():
                timestamp = int(time.time())
                new_name = f"{file_path.stem}_{timestamp}{file_path.suffix}"
                archive_file = archive_folder / new_name

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º shutil –¥–ª—è –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è
            import shutil
            shutil.copy2(file_path, archive_file)

            # –£–¥–∞–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è
            try:
                file_path.unlink()
                self.logger.info(
                    f"   üì¶ –ò—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω –≤ –∞—Ä—Ö–∏–≤ –∏ —É–¥–∞–ª–µ–Ω: {archive_file.relative_to(self.processed_folder)}")
            except:
                # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å, —Ö–æ—Ç—è –±—ã –∑–∞–∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–ª–∏
                self.logger.warning(
                    f"   üì¶ –ò—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω –≤ –∞—Ä—Ö–∏–≤, –Ω–æ –Ω–µ —É–¥–∞–ª–µ–Ω: {archive_file.relative_to(self.processed_folder)}")

        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∞—Ä—Ö–∏–≤–∞—Ü–∏–∏ —Ñ–∞–π–ª–∞ {file_path}: {e}")

    async def log_pipeline_result(self, result: Dict[str, Any]) -> None:
        """
        –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏.
        """
        try:
            log_file = self.log_folder / f"pipeline_log_{datetime.now().strftime('%Y-%m-%d')}.json"

            # –ß–∏—Ç–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ª–æ–≥–∏
            logs = []
            if log_file.exists():
                try:
                    with open(log_file, 'r', encoding='utf-8') as f:
                        logs = json.load(f)
                except:
                    logs = []

            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π –ª–æ–≥
            logs.append(result)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –ª–æ–≥–∏
            with open(log_file, 'w', encoding='utf-8') as f:
                json.dump(logs, f, ensure_ascii=False, indent=2)

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –ª–æ–≥–∏ –≤ S3 —Å –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ–º
            s3_log_path = (f"logs/"
                           f"pipeline_log_{datetime.now().strftime('%Y-%m-%d')}.json")
            await self.s3_client.upload(str(log_file), s3_log_path)
            await self.s3_client.upload_with_versioning(str(log_file), s3_log_path)

            self.logger.info(f"   üìã –õ–æ–≥–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {log_file.name} -> {s3_log_path}")

        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")

    async def process_existing_files(self) -> None:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ incoming.
        """
        files = list(self.watch_folder.glob("*.*"))
        if not files:
            self.logger.info("üì≠ –í –ø–∞–ø–∫–µ incoming –Ω–µ—Ç —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            return

        self.logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(files)}")

        for file_path in files:
            if file_path.is_file():
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –∏ –ª–æ–≥–∏
                if file_path.name.startswith(('.', '~', 'temp_')) or file_path.suffix in ['.log', '.tmp']:
                    continue

                result = await self.process_file(file_path)
                await self.log_pipeline_result(result)

                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Ñ–∞–π–ª–æ–≤
                await asyncio.sleep(1)
